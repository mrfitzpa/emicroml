{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb7f461",
   "metadata": {},
   "source": [
    "# Correcting distortion in SAED data #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d9cd7",
   "metadata": {},
   "source": [
    "## A NOTE BEFORE STARTING ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880421e9",
   "metadata": {},
   "source": [
    "Since the ``emicroml`` git repository tracks this notebook under its original\n",
    "basename ``correcting_distortion_in_saed_data.ipynb``, we recommend that you\n",
    "copy the original notebook and rename it to any other basename that is not one\n",
    "of the original basenames that appear in the ``<root>/examples`` directory\n",
    "before executing any of the notebook cells below, where ``<root>`` is the root\n",
    "of the ``emicroml`` repository. For example, you could rename it\n",
    "``correcting_distortion_in_saed_data.ipynb``. This way you can explore the\n",
    "notebook by executing and modifying cells without changing the original\n",
    "notebook, which is being tracked by git."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b0d90e",
   "metadata": {},
   "source": [
    "## Import necessary modules ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pattern matching.\n",
    "import re\n",
    "\n",
    "# For listing files and subdirectories in a given directory, and for renaming\n",
    "# directories.\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# For general array handling.\n",
    "import numpy as np\n",
    "\n",
    "# For creating and plotting figures.\n",
    "import hyperspy.api as hs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For minimizing objective functions.\n",
    "import scipy.optimize\n",
    "\n",
    "\n",
    "\n",
    "# For loading ML models for distortion estimation in CBED.\n",
    "import emicroml.modelling.cbed.distortion.estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d0485-cf65-478b-80c4-d5d294879c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667d633",
   "metadata": {},
   "source": [
    "## Introduction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f2509",
   "metadata": {},
   "source": [
    "In this notebook, we show how one can use the machine learning (ML) model that\n",
    "is trained as a result of executing the \"action\" described in the page [Training\n",
    "a machine learning\n",
    "model](https://mrfitzpa.github.io/emicroml/examples/modelling/cbed/distortion/estimation/train_ml_model_set.html)\n",
    "to correct distortion in selected area electron diffraction (SAED)\n",
    "data. Strictly speaking, this ML model is trained to estimate distortion in\n",
    "convergent beam electron diffraction (CBED) patterns. However, by exploiting the\n",
    "fact that distortions predominantly come from post-specimen lenses,\n",
    "e.g. projection lenses, we can estimate and correction distortion in SAED data\n",
    "as follows:\n",
    "\n",
    "1. Collect the target experimental SAED data;\n",
    "2. Modify only pre-specimen lenses to produce CBED data;\n",
    "3. Use ML model to estimate distortion field in CBED data;\n",
    "4. Correct distortion in SAED data using distortion field from step 3.\n",
    "\n",
    "We demonstrate steps 3 and 4 using pre-collected experimental SAED and CBED\n",
    "patterns of a calibration sample of single-crystal Au oriented in the \\[100\\]\n",
    "direction. This experimental data was collected on a modified Hitachi SU9000\n",
    "scanning electron microscope operated at 20 keV.\n",
    "\n",
    "You can find the documentation for the ``emicroml`` library\n",
    "[here](https://mrfitzpa.github.io/emicroml/_autosummary/emicroml.html).  It is\n",
    "recommended that you consult the documentation of this library as you explore\n",
    "the notebook. Moreover, users should execute the cells in the order that they\n",
    "appear, i.e. from top to bottom, as some cells reference variables that are set\n",
    "in other cells above them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9445a5e-76d1-4957-8399-c8638ab57d0c",
   "metadata": {},
   "source": [
    "## Loading and visualizing the SAED and CBED patterns ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e3c338-e3cb-424c-84c7-b5fa6527dd24",
   "metadata": {},
   "source": [
    "Let's load and visualize the target SAED pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991614a-3d4b-42f8-9acb-5e6f190959e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data_dir = \"../data\"\n",
    "filename = (path_to_data_dir \n",
    "            + \"/for_demo_of_distortion_correction_in_saed_data\"\n",
    "            + \"/distorted_saed_pattern.npy\")\n",
    "\n",
    "kwargs = {\"file\": filename}\n",
    "distorted_saed_pattern_image = np.load(**kwargs)\n",
    "\n",
    "kwargs = {\"data\": distorted_saed_pattern_image}\n",
    "distorted_saed_pattern_signal = hs.signals.Signal2D(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {\"axes_off\": True, \n",
    "          \"scalebar\": False, \n",
    "          \"colorbar\": False, \n",
    "          \"gamma\": 0.2,\n",
    "          \"cmap\": \"plasma\", \n",
    "          \"title\": \"\"}\n",
    "distorted_saed_pattern_signal.plot(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f66fa-5610-40f9-b17f-99bbadf281cb",
   "metadata": {},
   "source": [
    "This SAED pattern is subject to optical distortion which we want to correct. To\n",
    "do this, keeping the sample inside, we modify only the pre-specimen lenses to\n",
    "produce a CBED pattern which should be subject approximately to the same\n",
    "distortion.\n",
    "\n",
    "Let's load and visualize the target CBED pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b187307-faa4-4853-9999-e7e4bf1dab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = (path_to_data_dir \n",
    "            + \"/for_demo_of_distortion_correction_in_saed_data\"\n",
    "            + \"/distorted_cbed_pattern.npy\")\n",
    "\n",
    "kwargs = {\"file\": filename}\n",
    "distorted_cbed_pattern_image = np.load(**kwargs)\n",
    "\n",
    "kwargs = {\"data\": distorted_cbed_pattern_image}\n",
    "distorted_cbed_pattern_signal = hs.signals.Signal2D(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {\"axes_off\": True, \n",
    "          \"scalebar\": False, \n",
    "          \"colorbar\": False, \n",
    "          \"gamma\": 0.2,\n",
    "          \"cmap\": \"plasma\", \n",
    "          \"title\": \"\"}\n",
    "distorted_cbed_pattern_signal.plot(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602148b6-6317-4b5a-962b-37941cac76ad",
   "metadata": {},
   "source": [
    "## Estimating the distortion in the CBED pattern ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf6f43e-07af-40f7-a220-8a3d8254849b",
   "metadata": {},
   "source": [
    "Now let's load our ML model so that we can estimate the distortion in the CBED\n",
    "pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45db07e-a183-427c-b4dc-a71260cc49fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ml_model_state_dicts = path_to_data_dir + \"/ml_models/ml_model_0\"\n",
    "pattern = \"ml_model_at_lr_step_[0-9]*\\.pth\"\n",
    "largest_lr_step_idx = max([name.split(\"_\")[-1].split(\".\")[0]\n",
    "                           for name in os.listdir(path_to_ml_model_state_dicts)\n",
    "                           if re.fullmatch(pattern, name)])\n",
    "\n",
    "ml_model_state_dict_filename = \\\n",
    "    (path_to_ml_model_state_dicts\n",
    "     + \"/ml_model_at_lr_step_{}.pth\".format(largest_lr_step_idx))\n",
    "\n",
    "\n",
    "\n",
    "module_alias = emicroml.modelling.cbed.distortion.estimation\n",
    "kwargs = {\"ml_model_state_dict_filename\": ml_model_state_dict_filename,\n",
    "          \"device_name\": None}  # Default to CUDA device if available.\n",
    "ml_model = module_alias.load_ml_model_from_file(**kwargs)\n",
    "\n",
    "_ = ml_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f944d-1f00-437a-8896-645499067a14",
   "metadata": {},
   "source": [
    "With the ML model loaded, let's estimate the distortion in the CBED pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f4838-2bf4-4df7-bb38-b515894ccffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_grid_dims_in_pixels = distorted_cbed_pattern_image.shape\n",
    "distorted_cbed_pattern_images = distorted_cbed_pattern_image[None, :, :]\n",
    "\n",
    "kwargs = {\"cbed_pattern_images\": distorted_cbed_pattern_images,\n",
    "          \"sampling_grid_dims_in_pixels\": sampling_grid_dims_in_pixels}\n",
    "distortion_models = ml_model.predict_distortion_models(**kwargs)\n",
    "\n",
    "distortion_model = distortion_models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdd0855-b333-4d80-86b0-5873e3adc8c4",
   "metadata": {},
   "source": [
    "Note that any input distorted CBED pattern must have image dimensions, in units\n",
    "of pixels, equal to\n",
    "``2*(ml_model.core_attrs[\"num_pixels_across_each_cbed_pattern\"],)``. This is\n",
    "because a given ML model is trained for images of fixed dimensions, in units of\n",
    "pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d0ecd5-cb5c-4576-8dad-0117b52f8c60",
   "metadata": {},
   "source": [
    "Let's visualize the predicted distortion field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d643920-26da-4d75-899f-a11c8cf3afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_step = 16\n",
    "\n",
    "\n",
    "\n",
    "quiver_kwargs = {\"angles\": \"uv\",\n",
    "                 \"pivot\": \"middle\",\n",
    "                 \"scale_units\": \"width\"}\n",
    "\n",
    "\n",
    "\n",
    "attr_name = \"sampling_grid\"\n",
    "sampling_grid = getattr(distortion_model, attr_name)\n",
    "sampling_grid = (sampling_grid[0].numpy(), sampling_grid[1].numpy())\n",
    "\n",
    "X = sampling_grid[0][::slice_step, ::slice_step]\n",
    "Y = sampling_grid[1][::slice_step, ::slice_step]\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "attr_name = \"flow_field_of_coord_transform\"\n",
    "flow_field = getattr(distortion_model, attr_name)\n",
    "flow_field = (flow_field[0].numpy(), flow_field[1].numpy())\n",
    "\n",
    "U = flow_field[0][::slice_step, ::slice_step]\n",
    "V = flow_field[1][::slice_step, ::slice_step]\n",
    "\n",
    "kwargs = quiver_kwargs\n",
    "ax.quiver(X, Y, U, V, **kwargs)\n",
    "ax.set_title(\"Flow Field Of Coordinate Transformation\")\n",
    "ax.set_xlabel(\"fractional horizontal coordinate\")\n",
    "ax.set_ylabel(\"fractional vertical coordinate\")\n",
    "\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65364360-a6cd-4a0f-ad79-23e68905f0ba",
   "metadata": {},
   "source": [
    "## Correcting the distortion in the SAED pattern ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca339ea-5436-40bf-80c1-adf166c14f55",
   "metadata": {},
   "source": [
    "Let's use the predicted distortion model to correct the distortion in the SAED\n",
    "pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de7eb8-4f64-4f7a-8e26-bbfeaccbf3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = \\\n",
    "    {\"distorted_images\": distorted_saed_pattern_image[None, None, :, :]}\n",
    "undistorted_then_resampled_images = \\\n",
    "    distortion_model.undistort_then_resample_images(**kwargs)\n",
    "\n",
    "undistorted_saed_pattern_image = \\\n",
    "    undistorted_then_resampled_images[0, 0].numpy(force=True)\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {\"data\": undistorted_saed_pattern_image}\n",
    "undistorted_saed_pattern_signal = hs.signals.Signal2D(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {\"axes_off\": True, \n",
    "          \"scalebar\": False, \n",
    "          \"colorbar\": False, \n",
    "          \"gamma\": 0.2,\n",
    "          \"cmap\": \"plasma\", \n",
    "          \"title\": \"\"}\n",
    "undistorted_saed_pattern_signal.plot(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728eeaa-64c7-4827-bd2a-e163eb03bf31",
   "metadata": {},
   "source": [
    "## Assessing the accuracy of the distortion correction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d45ab6-c8fc-4e23-8d4a-217aff0361f9",
   "metadata": {},
   "source": [
    "We know that the sample is single-crystal Au oriented in the \\[100\\] direction,\n",
    "used for calibration. As such, in the absence of distortions, the zero-order\n",
    "Laue zone (ZOLZ) reflections should lie on a square lattice. Therefore, to\n",
    "assess the accuracy of the distortion correction, we can fit square lattices to\n",
    "both the distorted SAED pattern and the undistorted SAED pattern, and compare\n",
    "the errors of the fits.\n",
    "\n",
    "The first step is to locate the ZOLZ reflections that are sufficiently visible\n",
    "in the SAED patterns. We can do this by applying masks, peak-finding algorithms,\n",
    "and manual curation. Let's do this for the distorted SAED pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa1ab4-4434-4c4d-b142-b7c0c4b8d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_y, N_x = sampling_grid_dims_in_pixels\n",
    "\n",
    "\n",
    "\n",
    "L = 25\n",
    "R = N_x-335\n",
    "B = N_y-460\n",
    "T = 120\n",
    "\n",
    "rectangular_mask_image = np.zeros((N_y, N_x), dtype=bool)\n",
    "rectangular_mask_image[T:N_y-B, L:N_x-R] = True\n",
    "\n",
    "rectangular_mask_signal = hs.signals.Signal2D(data=rectangular_mask_image)\n",
    "\n",
    "\n",
    "\n",
    "masked_distorted_saed_pattern_signal = (distorted_saed_pattern_signal\n",
    "                                        * rectangular_mask_signal)\n",
    "\n",
    "kwargs = {\"axes_off\": True, \n",
    "          \"scalebar\": False, \n",
    "          \"colorbar\": False, \n",
    "          \"gamma\": 0.2,\n",
    "          \"cmap\": \"plasma\", \n",
    "          \"title\": \"\"}\n",
    "distorted_saed_pattern_signal.plot(**kwargs)\n",
    "\n",
    "kwargs = {\"method\": \"difference_of_gaussian\", \n",
    "          \"overlap\": 0, \n",
    "          \"threshold\": 0.0025, \n",
    "          \"min_sigma\": 1,\n",
    "          \"max_sigma\": 2,\n",
    "          \"interactive\": False, \n",
    "          \"show_progressbar\": False}\n",
    "find_peaks_result = masked_distorted_saed_pattern_signal.find_peaks(**kwargs)\n",
    "candidate_peak_locations = find_peaks_result.data[0][:, ::-1].tolist()\n",
    "\n",
    "candidate_peak_locations += ([335.5, 417.5],\n",
    "                             [269.5, 112.5],\n",
    "                             [342.0, 160.0],\n",
    "                             [354.0, 218.0], \n",
    "                             [368.0, 275.5], \n",
    "                             [384.0, 341.5])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "curation_instructions = {\"[287, 162]\": None,\n",
    "                         \"[236, 242]\": [235.5, 241], \n",
    "                         \"[262, 357]\": [261.5, 357.5], \n",
    "                         \"[248, 298]\": [248.5, 297.5]}\n",
    "\n",
    "zolz_reflections = tuple()\n",
    "for candidate_peak_location in candidate_peak_locations:\n",
    "    candidate_peak_location_as_str = str(candidate_peak_location)\n",
    "    if candidate_peak_location_as_str in curation_instructions:\n",
    "        key = candidate_peak_location_as_str\n",
    "        if curation_instructions[key] is None:\n",
    "            continue\n",
    "        else:\n",
    "            zolz_reflection = curation_instructions[key]\n",
    "    else:\n",
    "        zolz_reflection = candidate_peak_location\n",
    "    \n",
    "    kwargs = {\"color\": \"black\", \n",
    "              \"sizes\": 3, \n",
    "              \"offsets\": zolz_reflection}\n",
    "    marker = hs.plot.markers.Points(**kwargs)\n",
    "    distorted_saed_pattern_signal.add_marker(marker, permanent=False)\n",
    "\n",
    "    zolz_reflections += (zolz_reflection,)\n",
    "\n",
    "zolz_reflections_of_distorted_saed_pattern = zolz_reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d1d7c2-e2e5-4762-86b5-6c04cd660556",
   "metadata": {},
   "source": [
    "Now, let's do this for the undistorted SAED pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667a12f-d8f5-4cab-87d6-a640380f9a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 50\n",
    "R = N_x-330\n",
    "B = N_y-440\n",
    "T = 129\n",
    "\n",
    "rectangular_mask_image = np.zeros((N_y, N_x), dtype=bool)\n",
    "rectangular_mask_image[T:N_y-B, L:N_x-R] = True\n",
    "\n",
    "rectangular_mask_signal = hs.signals.Signal2D(data=rectangular_mask_image)\n",
    "\n",
    "\n",
    "\n",
    "masked_undistorted_saed_pattern_signal = (undistorted_saed_pattern_signal\n",
    "                                          * rectangular_mask_signal)\n",
    "\n",
    "kwargs = {\"axes_off\": True, \n",
    "          \"scalebar\": False, \n",
    "          \"colorbar\": False, \n",
    "          \"gamma\": 0.2,\n",
    "          \"cmap\": \"plasma\", \n",
    "          \"title\": \"\"}\n",
    "undistorted_saed_pattern_signal.plot(**kwargs)\n",
    "\n",
    "kwargs = {\"method\": \"difference_of_gaussian\", \n",
    "          \"overlap\": 0, \n",
    "          \"threshold\": 0.0025, \n",
    "          \"min_sigma\": 1,\n",
    "          \"max_sigma\": 2,\n",
    "          \"interactive\": False, \n",
    "          \"show_progressbar\": False}\n",
    "find_peaks_result = masked_undistorted_saed_pattern_signal.find_peaks(**kwargs)\n",
    "candidate_peak_locations = find_peaks_result.data[0][:, ::-1].tolist()\n",
    "\n",
    "candidate_peak_locations += ([334.5, 402.5],\n",
    "                             [261.5, 119.5],\n",
    "                             [332.0, 161.0],\n",
    "                             [347.5, 217.5],\n",
    "                             [362.5, 274.0],\n",
    "                             [377.5, 332.0])\n",
    "\n",
    "\n",
    "\n",
    "curation_instructions = {\"[282, 164]\": None,\n",
    "                         \"[235, 242]\": [235, 241], \n",
    "                         \"[263, 356]\": [262.5, 355.5], \n",
    "                         \"[248, 298]\": [248.5, 298], \n",
    "                         \"[305, 287]\": [305.5, 286.5], \n",
    "                         \"[137, 321]\": [136.5, 320.5]}\n",
    "\n",
    "zolz_reflections = tuple()\n",
    "for candidate_peak_location in candidate_peak_locations:\n",
    "    candidate_peak_location_as_str = str(candidate_peak_location)\n",
    "    if candidate_peak_location_as_str in curation_instructions:\n",
    "        key = candidate_peak_location_as_str\n",
    "        if curation_instructions[key] is None:\n",
    "            continue\n",
    "        else:\n",
    "            zolz_reflection = curation_instructions[key]\n",
    "    else:\n",
    "        zolz_reflection = candidate_peak_location\n",
    "    \n",
    "    kwargs = {\"color\": \"black\", \n",
    "              \"sizes\": 3, \n",
    "              \"offsets\": zolz_reflection}\n",
    "    marker = hs.plot.markers.Points(**kwargs)\n",
    "    undistorted_saed_pattern_signal.add_marker(marker, permanent=False)\n",
    "\n",
    "    zolz_reflections += (zolz_reflection,)\n",
    "\n",
    "zolz_reflections_of_undistorted_saed_pattern = zolz_reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f0ee3-f98c-45a1-be7a-e67ea3d70d44",
   "metadata": {},
   "source": [
    "Now we need to perform the fits. The objective function that we will minimize is\n",
    "the mean of the Euclidean distances squared between the ZOLZ reflections and\n",
    "their corresponding points on the square lattice fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c7a1a7-fbdd-4f35-879c-eb25a7956cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x, zolz_reflections):\n",
    "    u_O_x, u_O_y, b, theta = x\n",
    "\n",
    "    # u_0_x: fractional horizontal coordinate of origin of square lattice fit.\n",
    "    # u_0_y: fractional vertical coordinate of origin of square lattice fit.\n",
    "    # b: length of primitive lattice vector.\n",
    "    # theta: rotation applied to lattice.\n",
    "\n",
    "    N = N_y\n",
    "\n",
    "    result = 0.0\n",
    "\n",
    "    for (k_x, k_y) in zolz_reflections:\n",
    "        to_round = ((k_x-u_O_x)*np.cos(theta) + (k_y-u_O_y)*np.sin(theta)) / b\n",
    "        rounded = np.round(to_round)\n",
    "        result += (to_round-rounded)**2\n",
    "\n",
    "        to_round = (-(k_x-u_O_x)*np.sin(theta) + (k_y-u_O_y)*np.cos(theta)) / b\n",
    "        rounded = np.round(to_round)\n",
    "        result += (to_round-rounded)**2\n",
    "\n",
    "    result *= ((b/N)**2) / len(zolz_reflections)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca38570-133a-4214-8cab-bf3d88d19e42",
   "metadata": {},
   "source": [
    "We define the fitting error to be the square root of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea8106c-22f8-4481-85db-b5b54d72aae8",
   "metadata": {},
   "source": [
    "Let's fit the ZOLZ reflections of the distorted SAED pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187bd2a-18e0-4b07-8bbd-bff9f05aaab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "zolz_reflections = np.array(zolz_reflections_of_distorted_saed_pattern)\n",
    "differences = zolz_reflections[1:]-zolz_reflections[0]\n",
    "\n",
    "\n",
    "\n",
    "u_O_x_guess = 235.5\n",
    "u_O_y_guess = 241\n",
    "b_guess = np.linalg.norm(differences, axis=(1,)).min().item()\n",
    "theta_guess = np.arctan2(299-u_O_y_guess, 248-u_O_x_guess)\n",
    "\n",
    "initial_guesses = (u_O_x_guess,\n",
    "                   u_O_y_guess,\n",
    "                   b_guess,\n",
    "                   theta_guess)\n",
    "\n",
    "\n",
    "\n",
    "u_O_x_bounds = (25, 335)\n",
    "u_O_y_bounds = (120, 460)\n",
    "b_bounds = (b_guess-5, b_guess+5)\n",
    "theta_bounds = (0.9*theta_guess, 1.1*theta_guess)\n",
    "\n",
    "bounds = (u_O_x_bounds,\n",
    "          u_O_y_bounds,\n",
    "          b_bounds,\n",
    "          theta_bounds)\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {\"fun\": objective,\n",
    "          \"args\": (zolz_reflections,),\n",
    "          \"x0\": initial_guesses,\n",
    "          \"bounds\": bounds}\n",
    "minimization_result = scipy.optimize.minimize(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "u_O_x, u_O_y, b, theta = minimization_result.x\n",
    "\n",
    "u_O = np.array((u_O_x, u_O_y))\n",
    "b_1 = b*np.array((np.cos(theta), np.sin(theta)))\n",
    "b_2 = b*np.array((-np.sin(theta), np.cos(theta)))\n",
    "\n",
    "M = 3\n",
    "\n",
    "lattice_positions = tuple()\n",
    "for m_1 in range(-M, M+1):\n",
    "    for m_2 in range(-M, M+1):\n",
    "        lattice_position = (u_O + m_1*b_1 + m_2*b_2).tolist()\n",
    "        lattice_positions += (lattice_position,)\n",
    "\n",
    "        \n",
    "\n",
    "kwargs = {\"axes_off\": True, \n",
    "          \"scalebar\": False, \n",
    "          \"colorbar\": False, \n",
    "          \"gamma\": 0.2,\n",
    "          \"cmap\": \"plasma\", \n",
    "          \"title\": \"\"}\n",
    "distorted_saed_pattern_signal.plot(**kwargs)\n",
    "\n",
    "for lattice_position in lattice_positions:\n",
    "    kwargs = {\"color\": \"black\", \n",
    "              \"sizes\": 3, \n",
    "              \"offsets\": lattice_position}\n",
    "    marker = hs.plot.markers.Points(**kwargs)\n",
    "    distorted_saed_pattern_signal.add_marker(marker, permanent=False)\n",
    "\n",
    "\n",
    "\n",
    "error = np.sqrt(minimization_result.fun)\n",
    "msg = \"The error of the fit is: {}, in units of the image width.\".format(error)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb62cd28-39c2-40af-a191-e3ebfdf10598",
   "metadata": {},
   "source": [
    "The black dots in the figure directly above form the best square lattice fit.\n",
    "\n",
    "Now let's do the same fitting procedure for the undistorted SAED pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8875b9-ad4c-4e6b-be15-86c38a0a6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "zolz_reflections = np.array(zolz_reflections_of_undistorted_saed_pattern)\n",
    "differences = zolz_reflections[1:]-zolz_reflections[0]\n",
    "\n",
    "\n",
    "\n",
    "u_O_x_guess = 235\n",
    "u_O_y_guess = 241\n",
    "b_guess = np.linalg.norm(differences, axis=(1,)).min().item()\n",
    "theta_guess = np.arctan2(298-u_O_y_guess, 249-u_O_x_guess)\n",
    "\n",
    "initial_guesses = (u_O_x_guess,\n",
    "                   u_O_y_guess,\n",
    "                   b_guess,\n",
    "                   theta_guess)\n",
    "\n",
    "\n",
    "\n",
    "u_O_x_bounds = (50, 330)\n",
    "u_O_y_bounds = (129, 440)\n",
    "b_bounds = (b_guess-5, b_guess+5)\n",
    "theta_bounds = (0.9*theta_guess, 1.1*theta_guess)\n",
    "\n",
    "bounds = (u_O_x_bounds,\n",
    "          u_O_y_bounds,\n",
    "          b_bounds,\n",
    "          theta_bounds)\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {\"fun\": objective,\n",
    "          \"args\": (zolz_reflections,),\n",
    "          \"x0\": initial_guesses,\n",
    "          \"bounds\": bounds}\n",
    "minimization_result = scipy.optimize.minimize(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "u_O_x, u_O_y, b, theta = minimization_result.x\n",
    "\n",
    "u_O = np.array((u_O_x, u_O_y))\n",
    "b_1 = b*np.array((np.cos(theta), np.sin(theta)))\n",
    "b_2 = b*np.array((-np.sin(theta), np.cos(theta)))\n",
    "\n",
    "M = 3\n",
    "\n",
    "lattice_positions = tuple()\n",
    "for m_1 in range(-M, M+1):\n",
    "    for m_2 in range(-M, M+1):\n",
    "        lattice_position = (u_O + m_1*b_1 + m_2*b_2).tolist()\n",
    "        lattice_positions += (lattice_position,)\n",
    "\n",
    "        \n",
    "\n",
    "kwargs = {\"axes_off\": True, \n",
    "          \"scalebar\": False, \n",
    "          \"colorbar\": False, \n",
    "          \"gamma\": 0.2,\n",
    "          \"cmap\": \"plasma\", \n",
    "          \"title\": \"\"}\n",
    "undistorted_saed_pattern_signal.plot(**kwargs)\n",
    "\n",
    "for lattice_position in lattice_positions:\n",
    "    kwargs = {\"color\": \"black\", \n",
    "              \"sizes\": 3, \n",
    "              \"offsets\": lattice_position}\n",
    "    marker = hs.plot.markers.Points(**kwargs)\n",
    "    undistorted_saed_pattern_signal.add_marker(marker, permanent=False)\n",
    "\n",
    "\n",
    "\n",
    "error = np.sqrt(minimization_result.fun)\n",
    "msg = \"The error of the fit is: {}, in units of the image width.\".format(error)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a6939-c8ab-4197-8aa5-4d2802031349",
   "metadata": {},
   "source": [
    "As we can see both visually and from the lattice fit errors, our ML approach\n",
    "corrects an appreciable amount of the distortion in the SAED pattern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
